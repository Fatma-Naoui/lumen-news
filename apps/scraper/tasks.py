#tasks.py(og)
from celery import shared_task
from django.db import IntegrityError, OperationalError, connection
from django.db.models import Q
from django.utils import timezone
from datetime import timedelta
from .scraper import fetch_articles
from .models import Article
from .utils.embeddings import get_embedding_batch
from dateutil.parser import parse as parse_date
import logging
import json
import os
from .config import JSON_PATH

logger = logging.getLogger(__name__)


# =============================================================================
# TASK 1: FAST SCRAPING (No Embeddings)
# =============================================================================

@shared_task(bind=True, time_limit=300)  # 5 minute limit
def scrape_news_fast(self):
    """
    Scrape articles and insert them FAST without embeddings.
    Embeddings will be generated by separate task.
    
    WHY SEPARATE:
    - Scraping completes in 30-60 seconds
    - Doesn't wait for slow embedding generation
    - If embeddings fail, news still gets saved
    """
    logger.info("=== Starting fast news scraping ===")
    
    try:
        articles = fetch_articles()
        logger.info(f"üì• Fetched {len(articles)} articles from sources")
    except Exception as e:
        logger.error(f"‚ùå Failed to fetch articles: {e}", exc_info=True)
        raise self.retry(exc=e, countdown=60, max_retries=3)
    
    if not articles:
        logger.info("‚ÑπÔ∏è No articles fetched")
        return {"new": 0, "updated": 0}
    
    saved_count = 0
    updated_count = 0
    
    for i, a in enumerate(articles, start=1):
        title = a.get("title") or "[No title]"
        url = a.get("url")
        text = a.get("text") or ""
        category = a.get("category") or ""
        source = a.get("source") or ""
        summary = a.get("summary") or text[:300]
        published_at_raw = a.get("published_at")
        
        published_at = None
        if published_at_raw:
            try:
                published_at = parse_date(published_at_raw)
            except Exception:
                pass
        
        if not url:
            logger.warning(f"[{i}] ‚ö†Ô∏è Skipping article without URL")
            continue
        
        try:
            obj, created = Article.objects.update_or_create(
                url=url,
                defaults={
                    "title": title,
                    "text": text,
                    "category": category,
                    "source": source,
                    "summary": summary,
                    "published_at": published_at,
                    "embedding": None,  # No embedding yet
                    "status": "pending",
                }
            )
            
            if created:
                saved_count += 1
                logger.info(f"[{i}] ‚úÖ NEW: {title[:60]}")
            else:
                # Update if fields are missing
                updated = False
                if not obj.summary and summary:
                    obj.summary = summary
                    updated = True
                if not obj.text and text:
                    obj.text = text
                    updated = True
                
                if updated:
                    obj.save()
                    updated_count += 1
                    logger.info(f"[{i}] üîÑ UPDATED: {title[:60]}")
        
        except IntegrityError as ie:
            logger.warning(f"[{i}] ‚ö†Ô∏è IntegrityError: {ie}")
        except OperationalError as oe:
            logger.error(f"[{i}] ‚ùå Database error: {oe}", exc_info=True)
            raise self.retry(exc=oe, countdown=30)
        except Exception as e:
            logger.error(f"[{i}] ‚ùå Error: {e}", exc_info=True)
    
    # Clean up
    connection.close()
    
    result = {
        "new": saved_count,
        "updated": updated_count,
        "total": len(articles)
    }
    
    logger.info(f"‚úÖ Scraping complete: {saved_count} new, {updated_count} updated")
    
    # Trigger embedding generation for articles without embeddings
    # This runs AFTER scraping completes
    pending_count = Article.objects.filter(embedding__isnull=True).count()
    if pending_count > 0:
        logger.info(f"üöÄ Triggering embedding generation for {pending_count} articles")
        generate_embeddings.delay()
    
    return result


# =============================================================================
# TASK 2: EMBEDDING GENERATION (Separate, Can Run Independently)
# =============================================================================

@shared_task(bind=True, time_limit=1800, soft_time_limit=1700)  # 30 min limit
def generate_embeddings(self):
    """
    Generate embeddings for all articles that don't have them.
    Runs separately from scraping.

    WHY SEPARATE:
    - Can retry independently if it fails
    - Doesn't block scraping
    - Can process whatever is currently available
    - Worker can restart without affecting news collection
    """
    
    logger.info("=== Starting embedding generation ===")
    
    try:
        # 1Ô∏è‚É£ Select only recent articles (adjust 20 min if needed)
        recent_cutoff = timezone.now() - timedelta(minutes=20)
        articles_needing_embeddings = (
            Article.objects.filter(
                embedding__isnull=True,
                scraped_at__gte=recent_cutoff
            )
            .filter(Q(text__isnull=False) | Q(summary__isnull=False))
        )

        total = articles_needing_embeddings.count()
        if total == 0:
            logger.info("‚úÖ No new articles to embed")
            return {"processed": 0, "message": "Nothing recent to process"}

        logger.info(f"üìù Found {total} recent articles needing embeddings")

        # 2Ô∏è‚É£ Prepare texts
        texts_to_embed, valid_articles, skipped = [], [], 0
        for article in articles_needing_embeddings:
            text = article.summary or article.text
            if text and len(text.strip()) >= 50:
                texts_to_embed.append(text)
                valid_articles.append(article)
            else:
                skipped += 1

        if not texts_to_embed:
            logger.info(f"‚ÑπÔ∏è No valid texts (skipped {skipped})")
            return {"processed": 0, "skipped": skipped}

        logger.info(f"ü§ñ Generating embeddings for {len(texts_to_embed)} articles in chunks...")

        # 3Ô∏è‚É£ Chunked embedding for stability
        chunk_size = 50
        processed = 0
        articles_with_embeddings = []

        for i in range(0, len(texts_to_embed), chunk_size):
            chunk_texts = texts_to_embed[i:i + chunk_size]
            chunk_objects = valid_articles[i:i + chunk_size]
            chunk_num = i // chunk_size + 1
            total_chunks = (len(texts_to_embed) - 1) // chunk_size + 1

            logger.info(f"Processing chunk {chunk_num}/{total_chunks} ({len(chunk_texts)} articles)...")

            try:
                embeddings = get_embedding_batch(chunk_texts)
                if not embeddings or len(embeddings) != len(chunk_texts):
                    logger.error(f"‚ùå Chunk {chunk_num} failed or mismatched")
                    continue

                for obj, emb in zip(chunk_objects, embeddings):
                    obj.embedding = emb
                    obj.save(update_fields=['embedding'])
                    processed += 1
                    articles_with_embeddings.append({
                        "id": obj.id,
                        "title": obj.title,
                        "url": obj.url,
                        "embedding": emb,
                        "published_at": str(obj.published_at)
                    })

                logger.info(f"‚úÖ Chunk {chunk_num} complete ({len(embeddings)} saved)")

            except Exception as e:
                logger.error(f"‚ùå Chunk {chunk_num} failed: {e}", exc_info=True)
                continue

        # 4Ô∏è‚É£ Backup to JSONL
        if articles_with_embeddings:
            save_embeddings_to_json(articles_with_embeddings)

        # 5Ô∏è‚É£ Cleanup
        connection.close()
        remaining = Article.objects.filter(embedding__isnull=True).count()

        result = {
            "processed": processed,
            "skipped": skipped,
            "total_found": total,
            "remaining": remaining
        }

        logger.info(f"""
        ‚úÖ Embedding generation complete:
        - Processed: {processed}
        - Skipped: {skipped}
        - Remaining: {result['remaining']}
        """)

        return result

    except Exception as e:
        logger.error(f"‚ùå Embedding task failed: {e}", exc_info=True)
        connection.close()
        raise self.retry(exc=e, countdown=300, max_retries=3)



# =============================================================================
# HELPER: Save to JSONL
# =============================================================================

def save_embeddings_to_json(new_articles_with_embeddings):
    """Append new article embeddings to JSONL file"""
    os.makedirs(os.path.dirname(JSON_PATH), exist_ok=True)
    
    existing_ids = set()
    if os.path.exists(JSON_PATH):
        with open(JSON_PATH, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    item = json.loads(line.strip())
                    existing_ids.add(item["id"])
                except json.JSONDecodeError:
                    continue
    
    added_count = 0
    with open(JSON_PATH, "a", encoding="utf-8") as f:
        for article in new_articles_with_embeddings:
            if article["id"] not in existing_ids:
                f.write(json.dumps(article, ensure_ascii=False) + "\n")
                added_count += 1
    
    logger.info(f"‚úÖ Saved {added_count} embeddings to JSONL")


# =============================================================================
# OPTIONAL: Combined task (if you want both in one call)
# =============================================================================

@shared_task
def scrape_and_generate_embeddings():
    """
    Run both tasks in sequence.
    Use this if you want to trigger both at once.
    """
    logger.info("=== Running scrape + embeddings pipeline ===")
    
    # Step 1: Scrape (fast)
    scrape_result = scrape_news_fast.apply_async()
    scrape_result.get()  # Wait for completion
    
    # Step 2: Generate embeddings (slower)
    embed_result = generate_embeddings.apply_async()
    embed_result.get()  # Wait for completion
    
    return {
        "scraping": scrape_result.result,
        "embeddings": embed_result.result
    }